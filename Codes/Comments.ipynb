{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************* OBSERVATIONS ON THE OLS ANALYSIS **********************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) When performing the \"train error and test error VS model complexity\" the behavior changes with the number of datapoints, i.e. $\\\\$\n",
    "    i) few datapoints (around 400) --> train error decreases with the complexity, test error decreases at the beginning but then increases with the complexity of the  model; $\\\\$\n",
    "    ii) more datapoints (> 1000) --> both train and test errors decreases with the complexity. $\\\\$\n",
    "It might then make sense to run the worst-case study (also because there are usually only a few data points), i.e. for Ridge and Lasso we consider a fixed amount of datapoints (the worst one).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Bias-varaince trade-off and resampling techniques:\n",
    "    To this the bias variance analysis we need resampling techniques in order to compute an opproximation of the expected value of MSE, bias^2 and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. It appears that the graphs for MSE and the $R^2$ score are x-symmetric. I think this is because we are considering scaled data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. LASSO. Probably it is normal that behaves poorly. Reasons:\n",
    "- the outputs (y values) are quite small ( 0 < y < 1.40 ),\n",
    "- when \\beta is not zero, its contribution to the loss function should be +lambda or -lambda. Which in this case is a lot, leading to choose the model with all (or most) betas equal to zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********************************************************************** SECOND TASK **********************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) We are trying to model a polynomial. We can assume that we know the polynomial but not the coefficients, i.e. we know that the phonomenon is described by a polynomial\n",
    "$$ a_0 + a_1 x + a_2 x^2 + a_3 x^3 + a_4 x^4 + a_5 x^5  \\hspace{1cm} \\text{ with } a_0,a_1,a_2,a_3,a_4, a_5 \\in \\R$$\n",
    "but we do not know the values of the coefficents $a_0,a_1,a_2,a_3,a_4, a_5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Forse per i metodi in cui il learning rate cambia non mettere nella tabella il valore del learning rate ma nel report specificherei i valori di t0 e t1.\n",
    "This is because choosing (t0 = 0.001, t1 = 1) and (t0 = 1, t1 = 1000) will result in different learning rates from the second step, and therefore, different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Analogo al punto prima non metterei max number of interatio and this fixed values in the legend of the plot but in the comment of the plot in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "print('NOT SCALED DATA \\n')\n",
    "head_title = ['Learning rate', 'MSE train', 'MSE test']\n",
    "GD_table = [['{:.2e}'.format(GD_etas[0]), GD_MSE_train[0], GD_MSE_test[0]], \n",
    "            ['{:.2e}'.format(GD_etas[1]), GD_MSE_train[1], GD_MSE_test[1]], \n",
    "            ['{:.2e}'.format(GD_etas[2]), GD_MSE_train[2], GD_MSE_test[2]], \n",
    "            ['{:.2e}'.format(GD_etas[3]), GD_MSE_train[3], GD_MSE_test[3]],\n",
    "            ['OLS', OLS_MSE_train, OLS_MSE_test]]\n",
    "    \n",
    "print(tabulate(GD_table, headers=head_title, tablefmt='fancy_grid'))\n",
    "\n",
    "print('\\nSCALED DATA\\n')\n",
    "head_title = ['Learning rate', 'MSE train', 'MSE test']\n",
    "scaled_GD_table = [['{:.2e}'.format(scaled_GD_etas[0]), scaled_GD_MSE_train[0], scaled_GD_MSE_test[0]], \n",
    "            ['{:.2e}'.format(scaled_GD_etas[1]), scaled_GD_MSE_train[1], scaled_GD_MSE_test[1]], \n",
    "            ['{:.2e}'.format(scaled_GD_etas[2]), scaled_GD_MSE_train[2], scaled_GD_MSE_test[2]], \n",
    "            ['{:.2e}'.format(scaled_GD_etas[3]), scaled_GD_MSE_train[3], scaled_GD_MSE_test[3]],\n",
    "            ['OLS', scaled_OLS_MSE_train, scaled_OLS_MSE_test]]\n",
    "    \n",
    "print(tabulate(scaled_GD_table, headers=head_title, tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1\n",
    "b = 1\n",
    "a == b & b == 1\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('Project_python39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa94f5985e1f53257198865ff7f33ac4300f757c44ebe13fd3c6f0b5f057b01a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
